{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCBLANE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Conv1D, BatchNormalization, LSTM, Flatten, MultiHeadAttention, MaxPooling1D, Add, PReLU, SpatialDropout1D, Bidirectional, ZeroPadding1D, Multiply, Attention, AdditiveAttention, Concatenate\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "import keras\n",
    "# Input layers\n",
    "input_layer_1 = Input(shape=(101, 4), name='Input_Layer_1')\n",
    "input_layer_2 = Input(shape=(12, 16), name='Input_Layer_2')\n",
    "\n",
    "input_gnnbilstm_1 = Bidirectional(LSTM(64, return_sequences=True), merge_mode=\"sum\")(input_layer_2)\n",
    "input_gnnlstm_1 = LSTM(16, dropout=0.1)(input_gnnbilstm_1)\n",
    "\n",
    "\n",
    "# Convolutional Block 0\n",
    "convolution_layer_1 = Conv1D(filters = 256,kernel_size=8,padding=\"same\",name='Conv_0')(input_layer_1)\n",
    "convolution_layer_1 = PReLU(name='PReLU_0')(convolution_layer_1)\n",
    "convolution_layer_1 = SpatialDropout1D(0.01, name='SpatialDropout_0')(convolution_layer_1)\n",
    "convolution_layer_1 = MaxPooling1D(pool_size=1, name='MaxPooling_0')(convolution_layer_1)\n",
    "convolution_layer_1 = BatchNormalization(name='BatchNormalization_0')(convolution_layer_1)\n",
    "\n",
    "# Convolutional Block 1\n",
    "convolution_layer_2 = Conv1D(filters=128, kernel_size=4, padding=\"same\", name='Conv_1')(convolution_layer_1)\n",
    "convolution_layer_2 = PReLU(name='PReLU_1')(convolution_layer_2)\n",
    "convolution_layer_2 = SpatialDropout1D(0.01, name='SpatialDropout_1')(convolution_layer_2)\n",
    "convolution_layer_2 = MaxPooling1D(pool_size=1, name='MaxPooling_1')(convolution_layer_2)\n",
    "convolution_layer_2 = BatchNormalization(name='BatchNormalization_1')(convolution_layer_2)\n",
    "\n",
    "# Convolutional Block 2\n",
    "convolution_layer_3 = Conv1D(filters=64, kernel_size=2, padding=\"same\", name='Conv_2')(convolution_layer_2)\n",
    "convolution_layer_3 = PReLU(name='PReLU_2')(convolution_layer_3)\n",
    "convolution_layer_3 = SpatialDropout1D(0.01, name='SpatialDropout_2')(convolution_layer_3)\n",
    "convolution_layer_3 = MaxPooling1D(pool_size=2, name='MaxPooling_2')(convolution_layer_3)\n",
    "convolution_layer_3 = BatchNormalization(name='BatchNormalization_2')(convolution_layer_3)\n",
    "# 50\n",
    "\n",
    "# Convolutional Block 3\n",
    "convolution_layer_4 = Conv1D(filters=64, kernel_size=2, padding=\"same\", name='Conv_3')(convolution_layer_3)\n",
    "convolution_layer_4 = PReLU(name='PReLU_3')(convolution_layer_4)\n",
    "convolution_layer_4 = SpatialDropout1D(0.01, name='SpatialDropout_3')(convolution_layer_4)\n",
    "convolution_layer_4 = MaxPooling1D(pool_size=2, name='MaxPooling_3')(convolution_layer_4)\n",
    "convolution_layer_4 = BatchNormalization(name='BatchNormalization_3')(convolution_layer_4)\n",
    "# 25\n",
    "\n",
    "Query = Conv1D(filters=64, padding=\"same\", kernel_size=8, name=f'Query')(convolution_layer_4)\n",
    "\n",
    "# Multi-Head Attention\n",
    "heads = 8\n",
    "self_attention_layer, attention_scores = MultiHeadAttention(num_heads=heads,\n",
    "                                                            key_dim=32,\n",
    "                                                            value_dim=32,\n",
    "                                                            name='MultiHeadAttention')(\n",
    "                                                                query=Query,\n",
    "                                                                key=convolution_layer_4,\n",
    "                                                                value=convolution_layer_4,\n",
    "                                                                return_attention_scores=True)\n",
    "\n",
    "self_attention_layer = Multiply(name='SelfAttention_Multiplication')([self_attention_layer, convolution_layer_4])\n",
    "\n",
    "# Recurrent Block\n",
    "bilstm_layer = Bidirectional(LSTM(64, return_sequences=True), merge_mode=\"sum\", name='Bidirectional_LSTM')(self_attention_layer)\n",
    "lstm_layer = LSTM(64, dropout=0.1, name='LSTM')(bilstm_layer)\n",
    "\n",
    "concatenated_features = Concatenate()([input_gnnlstm_1, lstm_layer])\n",
    "\n",
    "# Output Block\n",
    "output_layer = Dense(2, activation=\"softmax\", name='finalOutput')(Flatten(name='flattenOutput')(concatenated_features))\n",
    "\n",
    "# Model definition\n",
    "GCBLANE = Model(inputs=[input_layer_1,input_layer_2], outputs=output_layer)\n",
    "\n",
    "GCBLANE.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',keras.metrics.AUC()])\n",
    "\n",
    "GCBLANE.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "@keras.saving.register_keras_serializable()\n",
    "def check_units(y_true, y_pred):\n",
    "    if y_pred.shape[1] != 1:\n",
    "      y_pred = y_pred[:,1:2]\n",
    "      y_true = y_true[:,1:2]\n",
    "    return y_true, y_pred\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "def precision(y_true, y_pred):\n",
    "    y_true, y_pred = check_units(y_true, y_pred)\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "def recall(y_true, y_pred):\n",
    "    y_true, y_pred = check_units(y_true, y_pred)\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "def f1(y_true, y_pred):\n",
    "    prec = precision(y_true, y_pred)\n",
    "    rec = recall(y_true, y_pred)\n",
    "    return 2*((prec*rec)/(prec+rec+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "class TrainDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, seq_features_path, gnn_features_path, labels_path, batch_size):\n",
    "        self.seq_features = np.load(seq_features_path)['train_sequences']\n",
    "        self.gnn_features = np.load(gnn_features_path)['arr_0']\n",
    "        self.labels = to_categorical(np.load(labels_path)['train_labels'],num_classes=2)\n",
    "        self.batch_size = batch_size\n",
    "        self.indexes = np.arange(len(self.seq_features))\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.seq_features) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        start = index * self.batch_size\n",
    "        end = (index + 1) * self.batch_size\n",
    "        batch_seq_features = self.seq_features[start:end]\n",
    "        batch_gnn_features = self.gnn_features[start:end]\n",
    "        batch_labels = self.labels[start:end]\n",
    "        return (batch_seq_features,batch_gnn_features), batch_labels\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indexes)\n",
    "        self.seq_features = self.seq_features[self.indexes]\n",
    "        self.gnn_features = self.gnn_features[self.indexes]\n",
    "        self.labels = self.labels[self.indexes]\n",
    "\n",
    "class testDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, seq_features_path, gnn_features_path, labels_path, batch_size):\n",
    "        self.seq_features = np.load(seq_features_path)['test_sequences']\n",
    "        self.gnn_features = np.load(gnn_features_path)['arr_0']\n",
    "        self.labels = to_categorical(np.load(labels_path)['test_labels'],num_classes=2)\n",
    "        self.batch_size = batch_size\n",
    "        self.indexes = np.arange(len(self.seq_features))\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.seq_features) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        start = index * self.batch_size\n",
    "        end = (index + 1) * self.batch_size\n",
    "        batch_seq_features = self.seq_features[start:end]\n",
    "        batch_gnn_features = self.gnn_features[start:end]\n",
    "        batch_labels = self.labels[start:end]\n",
    "        return (batch_seq_features, batch_gnn_features), batch_labels\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indexes)\n",
    "        self.seq_features = self.seq_features[self.indexes]\n",
    "        self.gnn_features = self.gnn_features[self.indexes]\n",
    "        self.labels = self.labels[self.indexes]\n",
    "        \n",
    "class validationDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, seq_features_path, gnn_features_path, labels_path, batch_size):\n",
    "        self.seq_features = np.load(seq_features_path)['validation_sequences']\n",
    "        self.gnn_features = np.load(gnn_features_path)['arr_0']\n",
    "        self.labels = to_categorical(np.load(labels_path)['validation_labels'],num_classes=2)\n",
    "        self.batch_size = batch_size\n",
    "        self.indexes = np.arange(len(self.seq_features))\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.seq_features) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        start = index * self.batch_size\n",
    "        end = (index + 1) * self.batch_size\n",
    "        batch_seq_features = self.seq_features[start:end]\n",
    "        batch_gnn_features = self.gnn_features[start:end]\n",
    "        batch_labels = self.labels[start:end]\n",
    "        return (batch_seq_features, batch_gnn_features), batch_labels\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indexes)\n",
    "        self.seq_features = self.seq_features[self.indexes]\n",
    "        self.gnn_features = self.gnn_features[self.indexes]\n",
    "        self.labels = self.labels[self.indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_signature = (\n",
    "    (\n",
    "        tf.TensorSpec(shape=(None, 101, 4), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None, 12, 16), dtype=tf.float32)\n",
    "    ),\n",
    "    tf.TensorSpec(shape=(None, 2), dtype=tf.float32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "gnn_train_dirs = sorted([f for f in os.listdir(\"/content/content/final_data/train/\") if f.startswith(\"gnn\") and f.endswith(\".npz\")])\n",
    "seq_train_dirs = sorted([f for f in os.listdir(\"/content/content/final_data/train/\") if f.startswith(\"seq\") and f.endswith(\".npz\")])\n",
    "\n",
    "gnn_test_dirs = sorted([f for f in os.listdir(\"/content/content/final_data/test/\") if f.startswith(\"gnn\") and f.endswith(\".npz\")])\n",
    "seq_test_dirs = sorted([f for f in os.listdir(\"/content/content/final_data/test/\") if f.startswith(\"seq\") and f.endswith(\".npz\")])\n",
    "\n",
    "metrics = {}\n",
    "\n",
    "for i in range(690):\n",
    "    gnn_train_dirs[i] = \"/content/content/final_data/train/\" + gnn_train_dirs[i]\n",
    "    seq_train_dirs[i] = \"/content/content/final_data/train/\" + seq_train_dirs[i]\n",
    "    gnn_test_dirs[i] = \"/content/content/final_data/test/\" + gnn_test_dirs[i]\n",
    "    seq_test_dirs[i] = \"/content/content/final_data/test/\" + seq_test_dirs[i]\n",
    "\n",
    "    batch_size = 128\n",
    "\n",
    "    test_generator = testDataGenerator(seq_test_dirs[i], gnn_test_dirs[i], seq_test_dirs[i], batch_size)\n",
    "\n",
    "    test_dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: test_generator,\n",
    "        output_signature=output_signature\n",
    "    ).repeat()\n",
    "\n",
    "    train_generator = TrainDataGenerator(seq_train_dirs[i], gnn_train_dirs[i], seq_train_dirs[i], batch_size)\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: train_generator,\n",
    "        output_signature=output_signature\n",
    "    ).repeat()\n",
    "\n",
    "    GCBLANE = load_model(\"GCBLANE.keras\")\n",
    "\n",
    "    GCBLANE.compile(optimizer=Adam(0.001),\n",
    "                        loss='categorical_crossentropy',\n",
    "                        metrics = [\n",
    "                            keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
    "                             precision,\n",
    "                             recall,\n",
    "                            keras.metrics.AUC(name='PRAUC', curve='PR'),\n",
    "                            keras.metrics.AUC(name='ROCAUC', curve='ROC'),\n",
    "                             f1\n",
    "                            ])\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        filepath='best_model.keras',\n",
    "        monitor='val_ROCAUC',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    early_stopping_callback = EarlyStopping(\n",
    "        monitor='val_ROCAUC',\n",
    "        patience=3,\n",
    "        mode='max',\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "\n",
    "    # Fit the model with the callbacks\n",
    "    hist = GCBLANE.fit(train_dataset,\n",
    "                       steps_per_epoch=len(train_generator),\n",
    "                       epochs=50,\n",
    "                       validation_data=test_dataset,\n",
    "                       validation_steps=len(test_generator),\n",
    "                       callbacks=[checkpoint_callback, early_stopping_callback],\n",
    "                       verbose = 1\n",
    "                       )\n",
    "\n",
    "    best_model = load_model('best_model.keras')\n",
    "    metrics[i] = best_model.evaluate(test_dataset, steps=len(test_generator), verbose=1)\n",
    "    print(metrics[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_graphs = np.load('/content/train_graph_data.npz')\n",
    "validation_graphs = np.load('/content/validation_graph_data.npz')\n",
    "test_graphs = np.load('/content/test_graph_data.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spektral.data import Graph, Dataset\n",
    "class DNADataset(Dataset):\n",
    "    def __init__(self, graph_list):\n",
    "        self.node_features = graph_list['node_features']\n",
    "        self.adj = graph_list['adj']\n",
    "        self.labels = graph_list['labels']\n",
    "        super().__init__()\n",
    "\n",
    "    def read(self):\n",
    "        graphs = []\n",
    "        for i in range(len(self.node_features)):\n",
    "            graphs.append(Graph(x=self.node_features[i].astype(np.float16), a=self.adj[i].astype(np.float16), y = self.labels[i]))\n",
    "        return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DNADataset(train_graphs)\n",
    "validation_dataset = DNADataset(validation_graphs)\n",
    "test_dataset = DNADataset(test_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from spektral.data import BatchLoader, PackedBatchLoader\n",
    "from spektral.layers import GCNConv, GlobalMaxPool, MinCutPool\n",
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "\n",
    "@register_keras_serializable()\n",
    "class GNNModel(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = GCNConv(128, activation='relu')\n",
    "        self.mincutpool1 = MinCutPool(40)\n",
    "        self.conv2 = GCNConv(64, activation='relu')\n",
    "        self.mincutpool2 = MinCutPool(12)\n",
    "        self.conv3 = GCNConv(16, activation='relu')\n",
    "\n",
    "        self.maxpool = GlobalMaxPool()\n",
    "        self.dense1 = Dense(32, activation='relu')\n",
    "        self.dense2 = Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, a = inputs\n",
    "        x = self.conv1([x, a])\n",
    "\n",
    "        x, a = self.mincutpool1([x, a])\n",
    "        x = self.conv2([x, a])\n",
    "\n",
    "        x, a = self.mincutpool2([x, a])\n",
    "        x = self.conv3([x, a])\n",
    "\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GNN_Model = GNNModel()\n",
    "\n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint(filepath='gnn_model.keras',\n",
    "                                                      monitor='val_accuracy',\n",
    "                                                      mode='max',\n",
    "                                                      save_best_only=True)\n",
    "\n",
    "\n",
    "GNN_Model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = BatchLoader(train_dataset, batch_size=128)\n",
    "validation_loader = BatchLoader(validation_dataset, batch_size=128)\n",
    "test_loader = BatchLoader(test_dataset, batch_size=128)\n",
    "\n",
    "GNN_Model.fit(train_loader.load(),\n",
    "              steps_per_epoch=train_loader.steps_per_epoch,\n",
    "              epochs=20,\n",
    "              validation_data=validation_loader.load(),\n",
    "              validation_steps=validation_loader.steps_per_epoch\n",
    "              )\n",
    "\n",
    "GNN_Model.evaluate(test_loader.load(),\n",
    "                   steps=test_loader.steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from spektral.layers import GCNConv, GlobalMaxPool, MinCutPool\n",
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "\n",
    "@register_keras_serializable()\n",
    "class GNNModelPooled(Model):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = GCNConv(128, activation='relu')\n",
    "        self.mincutpool1 = MinCutPool(40)\n",
    "        self.conv2 = GCNConv(64, activation='relu')\n",
    "        self.mincutpool2 = MinCutPool(12)\n",
    "        self.conv3 = GCNConv(16, activation='relu')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, a = inputs\n",
    "\n",
    "        x = self.conv1([x, a])\n",
    "        x, a = self.mincutpool1([x, a])\n",
    "        x = self.conv2([x, a])\n",
    "        x, a = self.mincutpool2([x, a])\n",
    "        x = self.conv3([x, a])\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCBLANEGNN = GNNModelPooled(GNNModel())\n",
    "\n",
    "GNN_Model = load_model(\"gnn.keras\")\n",
    "\n",
    "GCBLANEGNN.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "try:\n",
    "  GCBLANEGNN.fit(validation_loader.load(),\n",
    "              steps_per_epoch=validation_loader.steps_per_epoch,\n",
    "              epochs=1)\n",
    "except:\n",
    "  pass\n",
    "\n",
    "GCBLANEGNN.layers[0].set_weights(GNN_Model.layers[0].get_weights())\n",
    "GCBLANEGNN.layers[1].set_weights(GNN_Model.layers[1].get_weights())\n",
    "GCBLANEGNN.layers[2].set_weights(GNN_Model.layers[2].get_weights())\n",
    "GCBLANEGNN.layers[3].set_weights(GNN_Model.layers[3].get_weights())\n",
    "GCBLANEGNN.layers[4].set_weights(GNN_Model.layers[4].get_weights())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
